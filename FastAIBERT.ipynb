{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.60'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "config = Config(\n",
    "    testing=False,\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    max_lr=1e-2,\n",
    "    epochs=1,\n",
    "    use_fp16=False,\n",
    "    bs=32,\n",
    "    discriminative=False,\n",
    "    max_seq_len=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    config.bert_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]=BOS):\n",
    "    \"\"\"Borrowed from fast.ai source\"\"\"\n",
    "    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n",
    "    if is1d(texts): texts = texts[:,None]\n",
    "    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n",
    "    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n",
    "    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n",
    "    for i in range(1,len(df.columns)):\n",
    "        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n",
    "        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n",
    "    return text_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 2)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = \"C:\\\\Users\\\\osrivast\\\\Desktop\\\\Everything_work\\\\Research\\\\BERTvsULMFit\\\\dataset\"\n",
    "src_fol = Path(location)\n",
    "#Decide the train size\n",
    "file_train = \"trainset.xlsx\"\n",
    "datafile = file_train\n",
    "\n",
    "train = pd.read_excel(datafile)\n",
    "train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1649, 2)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 2)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_test = \"testsetBERT.xlsx\"\n",
    "datafile_test = file_test\n",
    "\n",
    "test = pd.read_excel(datafile_test)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/osrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = train['text'].apply(lambda x: x.split())\n",
    "\n",
    "\n",
    "\n",
    "# remove stop-words \n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization \n",
    "detokenized_doc = [] \n",
    "for i in range(len(train)): \n",
    "    t = ' '.join(tokenized_doc[i]) \n",
    "    detokenized_doc.append(t) \n",
    "    \n",
    "train['text'] = detokenized_doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1319, 2), (330, 2))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#We know the training size here\n",
    "train, val = train_test_split(train, stratify = train['category'], test_size = 0.2, random_state = 12)\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_cols = ['category']\n",
    "label_cols = ['business', 'entertainment', 'politics', 'sport', 'tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for BERT\n",
    "    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original BERT model.\n",
    "    \"\"\"\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "databunch = TextDataBunch.from_df(\".\", train, val, \n",
    "                                  test, tokenizer=fastai_tokenizer, \n",
    "                                  vocab=fastai_bert_vocab, \n",
    "                                  bs=32, include_bos=False,\n",
    "     include_eos=False, collate_fn=partial(pad_collate, pad_first=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
    "bert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = nn.BCEWithLogitsLoss()\n",
    "#loss_func = nn.NLLLoss\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "\n",
    "learner = Learner(\n",
    "    databunch, bert_model,\n",
    "    loss_func=loss_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.175191</td>\n",
       "      <td>2.630991</td>\n",
       "      <td>18:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.916630</td>\n",
       "      <td>1.806864</td>\n",
       "      <td>11:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.739515</td>\n",
       "      <td>1.919592</td>\n",
       "      <td>13:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.258651</td>\n",
       "      <td>1.727643</td>\n",
       "      <td>12:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.909431</td>\n",
       "      <td>1.605483</td>\n",
       "      <td>12:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#LR for LM and epochs\n",
    "#Takes like 10 mins\n",
    "learner.fit_one_cycle(5, 1e-2, moms=(0.8,0.7), wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.053079</td>\n",
       "      <td>1.708304</td>\n",
       "      <td>10:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner_safety = learner\n",
    "#Takes 15 mins per epoch\n",
    "#LR for classifier and epoch\n",
    "learner.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.60'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds, y = learner.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(tensor(0.1394), tensor(0.0791), tensor(0.2443), tensor(0.2557), tensor(0.2815))</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0                                                0   1  2   3  4\n",
       "row_0                                                               \n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   1  0   1  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   5   8  4   3  4\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   1  0   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  1   0  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   1  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  1   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   2  1   1  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   5   3  3   6  7\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  2   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  1   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   4   1  8   7  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   3   0  1   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...  13  10  6  10  8\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   6   6  5   7  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  1   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   2   1  1   1  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  2   0  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   2   1  2   1  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  2   0  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   3  0   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   5   6  2   8  2\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   1  0   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   7   6  7   4  3\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   7   5  1   6  3\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   0  3\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   6   2  5   8  3\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   1  0   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  1   1  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   1  4   0  1\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  0   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   1   0  0   0  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0\n",
       "(tensor(0.1394), tensor(0.0791), tensor(0.2443)...   0   0  0   1  0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(predictions, y)\n",
    "recall = recall_score(predictions, y, average='macro')\n",
    "f1 = f1_score(predictions, y, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC_ROC_CURVE for text classifier\n",
    "from fastai import metrics\n",
    "metrics.auc_roc_score(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17575757575757575, 0.03515151515151515, 0.059793814432989686)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy, recall, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
